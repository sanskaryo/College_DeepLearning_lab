{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Synonym Replacement"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Corpus : Collection of all words in documents\n",
        "\n",
        "Vocabulary : All unique words in corpus\n",
        "\n",
        "nltk : Natural Language Toolkit, python library for text processing\n",
        "\n",
        "wordnet : Lexical Database of english words in nltk\n",
        "\n",
        "omw-1.4 : Enabling multi-lingual support\n",
        "\n",
        "punkt : For tokenization"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting nltk\n  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hRequirement already satisfied: click in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (1.2.0)\nRequirement already satisfied: tqdm in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (4.67.1)\nRequirement already satisfied: regex>=2021.8.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from nltk) (2024.11.6)\nInstalling collected packages: nltk\nSuccessfully installed nltk-3.9.1\n"
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package wordnet to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1754290018614
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"The movie was absolutely fantastic and enjoyable\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synonyms(word):\n",
        "    # Set used to remove duplicate words, and only contain unique words\n",
        "    synonyms = set()\n",
        "\n",
        "    # Get all synsets(meanings) of the word from wordnet\n",
        "    for syn in wordnet.synsets(word):\n",
        "        # Each synset has multiple lemmas (lemmas are words with same meaning)\n",
        "        for lemma in syn.lemmas():\n",
        "            synonym = lemma.name().replace('_', ' ')\n",
        "\n",
        "        if synonym.lower() != word.lower():\n",
        "            synonyms.add(synonym)\n",
        "\n",
        "    return list(synonyms)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1754289889315
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def synonym_replacement(text, n=2):\n",
        "    # Tokenize sentence into words\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # We could also use split function, or spacy library which is the best (eg. handles terms like 5km, New Delhi etc.)\n",
        "\n",
        "    # Make a copy of the words to modify\n",
        "    new_words = words.copy()\n",
        "\n",
        "    random_word_list = list(set([word for word in words if word.isalpha()]))\n",
        "\n",
        "    random.shuffle(random_word_list)\n",
        "\n",
        "    # Keep track of how many words replaced\n",
        "    num_replaced = 0\n",
        "\n",
        "    # Go through all words\n",
        "    for word in random_word_list:\n",
        "        synonyms = get_synonyms(word)\n",
        "\n",
        "        # Chose a random synonym if exist\n",
        "        if synonyms:\n",
        "            synonym = random.choice(synonyms)\n",
        "\n",
        "            new_words = [synonym if w == word else w for w in new_words]\n",
        "\n",
        "            num_replaced += 1\n",
        "\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "    \n",
        "    return ' '.join(new_words)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1754289891383
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original = \"The movie was absolutely fantastic and enjoyable\"\n",
        "augmented = synonym_replacement(original, n=2)\n",
        "\n",
        "print(f\"{original=} and {augmented=}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "original='The movie was absolutely fantastic and enjoyable' and augmented='The movie was absolutely wondrous and pleasurable'\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1754290027966
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bigram_flip(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Make a copy of tokenized words as we don't use original directly\n",
        "    new_words = words.copy()\n",
        "\n",
        "    # List of indexes where each index represents first word of a bigram (pair of words)\n",
        "    indices = list(range(len(words) - 1))\n",
        "\n",
        "    # If less than 2 words in text, no biagrams will exist so we can't flip any bigram\n",
        "    if not indices:\n",
        "        return text\n",
        "\n",
        "    flip_index = random.choice(indices)\n",
        "\n",
        "    new_words[flip_index], new_words[flip_index+1] = new_words[flip_index+1], new_words[flip_index]\n",
        "\n",
        "    return ' '.join(new_words)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1754290409122
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "original2 = \"The movie was absolutely fantastic and enjoyable\"\n",
        "augmented2 = bigram_flip(original2)\n",
        "\n",
        "print(f\"{original2=} and {augmented2=}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "original2='The movie was absolutely fantastic and enjoyable' and augmented2='The movie was absolutely and fantastic enjoyable'\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1754290549148
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Back Translation"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep_translator"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting deep_translator\n  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m857.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from deep_translator) (4.13.4)\nRequirement already satisfied: requests<3.0.0,>=2.23.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from deep_translator) (2.32.3)\nRequirement already satisfied: soupsieve>1.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.6)\nRequirement already satisfied: typing-extensions>=4.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.13.2)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.1.31)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (1.26.20)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.1)\nInstalling collected packages: deep_translator\nSuccessfully installed deep_translator-1.11.4\n"
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We translate a text into another language and translate it back to original language, this can cause change in sentence."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "def back_translate_verbose(text, intermediate_lang='fr'):\n",
        "    try:\n",
        "        translated = GoogleTranslator(source='auto', target=intermediate_lang).translate(text)\n",
        "\n",
        "        back_translated = GoogleTranslator(source='auto', target='en').translate(translated)\n",
        "\n",
        "        print(f\"Original: {text}\")\n",
        "        print(f\"Translated: ({intermediate_lang}): {translated}\")\n",
        "        print(f\"Back Translated (English):{back_translated}\")\n",
        "\n",
        "        return back_translated\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(\"Translation error: \", e)\n",
        "        return text"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1754291020042
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original3 = \"The movie was absolutely fantastic and enjoyable\"\n",
        "augmented3 = back_translate_verbose(original3, intermediate_lang='fr')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Original: The movie was absolutely fantastic and enjoyable\nTranslated: (fr): Le film était absolument fantastique et agréable\nBack Translated (English):The film was absolutely fantastic and pleasant\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1754291024479
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Adding Noise"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a. Random character swaps"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise(text, noise_level=0.1):\n",
        "    \n",
        "    # Convert string to list of characters as list is immutale\n",
        "    text_chars = list(text)\n",
        "\n",
        "    # Calculate number of noisy operations to perform\n",
        "    num_noisy = int(len(text_chars) * noise_level)\n",
        "\n",
        "    # Loop to perform num_noisy times random adjacent swaps\n",
        "    for _ in range(num_noisy):\n",
        "\n",
        "        # Select a random index, ensuring there's a next character to swap with\n",
        "        idx = random.randint(0, len(text_chars) - 2)\n",
        "\n",
        "        # Swap characters\n",
        "        text_chars[idx], text_chars[idx+1] = text_chars[idx+1], text_chars[idx]\n",
        "\n",
        "    # We won't add spaces to join the characters, or extra space characters will be introduced, we want length of original and augmented sentence to be the same\n",
        "    return ''.join(text_chars)"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1754291766714
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original4 = \"The movie was absolutely fantastic and enjoyable\"\n",
        "augmented4 = add_noise(original4, noise_level=0.1)\n",
        "\n",
        "print(f\"{original4=} and {augmented4=}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "original4='The movie was absolutely fantastic and enjoyable' and augmented4='Th emovie was absolutley fantastic and enjoyable'\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1754291820152
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.10 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}